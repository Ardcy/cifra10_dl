# cifra10_dl

选择深度学习框架： PyTorch

## 数据集：CIFAR-10 数据集

## baseline: resnet18

##Prerequisites

* Python 3.6+
* PyTorch 1.0+

## 模型介绍

本模型通过使用resnet18模型，在CIFAR-10 数据集上进行训练，以及超参数设计，进行了一系列实验。

具体代码详看 main.py 文件。

>实验设置随机种子，以便模型网络结构复现

## Training

```shell
python main.py --epoch=200 --lr=0.1  --batch_size=256
```

> 除 epoch, lr, batch_size 参数外，模型设置了额外参数，--resume 方便模型加载ckpt文件，继续训练

## 模型训练过程

1. 第一步，直接训练，按照默认参数

   | epoch | learningrate | batch_size |
   | :---: | :----------: | :--------: |
   |  200  |     0.1      |    256     |

   ```shell
   Epoch: 199
    [=========================== 196/196 ============================>]  Step: 29ms | Tot: 11s700ms | Loss: 0.002 | Acc: 100.000% (50000/50000)                                           
    [=========================== 100/100 ============================>]  Step: 10ms | Tot: 1s198ms | Loss: 0.181 | Acc: 94.980% (9498/10000)                                              
   训练结束，耗时 2694 s
   ```

   训练200个epoch 耗时 2694s， 最终已经在训练集上正确率达到了100%，在测试集上正确率达到了94.98%

2. 整理模型结果

   <img src="README.assets/image-20211018144623731.png" alt="image-20211018144623731" style="zoom: 50%;" />

   当训练到137代时，模型训练集上的正确率已经达到了100%, 而之后通过不断训练，模型在测试集上的正确率还在不断增加，正确率从92%增长到了95%。

   模型的loss曲线如下图所示：

   ![image-20211018145102283](README.assets/image-20211018145102283.png)

   模型初期在训练集较大，迭代到 3-5 代时模型在训练集和测试集的 loss 已经开始保持一致。

   在第 20 代时，模型的 loss 值开始出现变化，在测试集上的 loss 变化很大，而在训练集上的变化则比较平稳；同时，loss 值在测试集上的大小明显要大于训练集的 loss.

   在 20-150 代区间的变化情况仍旧是 训练集变化平稳，测试集变化较大.

   模型在150代之后，训练集正确率到达100%，测试集正确率缓慢上升，loss值下降速度趋于0，不再发生明显的变化。

3. idea: 通过观察模型，发现模型在100代往后逐渐进入稳定的收敛过程，为探究学习率对稳定收敛的影响，我在100个epoch的情况下，加大学习率，设置为5倍默认学习率（即在scheduler模式下，扩大5倍学习率），在150个epoch时，重新调整学习率为默认学习率，观察实验结果。

   其中 scheduler模式下（具体过程详见代码）的实际学习率为：

   ![image-20211018154835876](README.assets/image-20211018154835876.png)

​        在200次迭代过程中，学习率逐渐下降为0.

​		通过实际实验，发现，学习率的骤增会极大影响测试正确率，当增大5倍时，

   ```shell
   Epoch: 48
    [=========================== 196/196 ============================>]  Step: 27ms | Tot: 11s670ms | Loss: 0.207 | Acc: 92.832% (46416/50000)                                            
    [=========================== 100/100 ============================>]  Step: 11ms | Tot: 1s197ms | Loss: 0.348 | Acc: 89.110% (8911/10000)                                              
   Saving..
   
   Epoch: 49
    [=========================== 196/196 ============================>]  Step: 28ms | Tot: 11s794ms | Loss: 0.203 | Acc: 92.956% (46478/50000)                                            
    [=========================== 100/100 ============================>]  Step: 9ms | Tot: 1s85ms | Loss: 0.418 | Acc: 87.000% (8700/10000)      
   
   学习率改变后的结果：
   
   Epoch: 50
    [=========================== 196/196 ============================>]  Step: 31ms | Tot: 11s741ms | Loss: 0.382 | Acc: 87.032% (43516/50000)                                            
    [=========================== 100/100 ============================>]  Step: 10ms | Tot: 1s206ms | Loss: 0.537 | Acc: 82.350% (8235/10000)                                              
   
   Epoch: 51
    [=========================== 196/196 ============================>]  Step: 29ms | Tot: 11s704ms | Loss: 0.360 | Acc: 87.704% (43852/50000)                                            
    [=========================== 100/100 ============================>]  Step: 9ms | Tot: 1s208ms | Loss: 0.564 | Acc: 81.860% (8186/10000)                                               
   
   Epoch: 52
    [=========================== 196/196 ============================>]  Step: 29ms | Tot: 11s800ms | Loss: 0.348 | Acc: 88.070% (44035/50000)                                            
    [=========================== 100/100 ============================>]  Step: 9ms | Tot: 1s279ms | Loss: 0.493 | Acc: 83.140% (8314/10000)  
   ```

​	由此，本次实验中，仅仅扩大学习率2倍，再次重新实验观察实验结果。

​	50 - 70代后的结果为

```shell
Epoch: 70
 [=========================== 196/196 ============================>]  Step: 28ms | Tot: 11s767ms | Loss: 0.271 | Acc: 90.778% (45389/50000)                                            
 [=========================== 100/100 ============================>]  Step: 10ms | Tot: 1s223ms | Loss: 0.495 | Acc: 84.120% (8412/10000) 
```

​	验证集上的正确率为84.120%，相对第48代的89.110%,仍然差距不小。同时，进行50个epoch后，模型的正确率仍然没有超过前50个epoch的最高正确率。仍然有1%左右的差距。

最终，改为缩小学习率为之前的1/2，迭代50个epoch, 再次重新实验观察结果。

```shell
Epoch: 96
 [=========================== 196/196 ============================>]  Step: 26ms | Tot: 11s827ms | Loss: 0.050 | Acc: 98.320% (49160/50000)                                            
 [=========================== 100/100 ============================>]  Step: 10ms | Tot: 1s190ms | Loss: 0.316 | Acc: 91.980% (9198/10000)
```

50个epoch之后 精确率达到了92%左右。

然后最后的100个epoch重新调整回最开始的学习率，得到最终的结果。

```shell
Epoch: 200
 [=========================== 196/196 ============================>]  Step: 29ms | Tot: 11s725ms | Loss: 0.002 | Acc: 99.998% (49999/50000)                                            
 [=========================== 100/100 ============================>]  Step: 11ms | Tot: 1s191ms | Loss: 0.178 | Acc: 95.130% (9513/10000) 
```

训练结束后的最高测试集正确率达到95.160%，在第180代达到。

数据结果如下图所示：



![image-20211018164500536](README.assets/image-20211018164500536.png)

![image-20211018164610806](README.assets/image-20211018164610806.png)

通过微调学习率，使得实验结果从94.98%上升到了95.160%。
